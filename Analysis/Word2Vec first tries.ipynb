{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###copied over from HW2 assignment, lazy way of getting a corpus with UNKS\n",
    "class Dataset:\n",
    "    def __init__(self, link):\n",
    "        self.all_tweets=self.read_data(link)\n",
    "        #if train data, get all the necessary things\n",
    "        self.clean_tweets, self.vocab, self.pairs=self.get_clean_datasets()\n",
    "        self.only_words=[[pair[0] for pair in tweet] for tweet in self.clean_tweets]\n",
    "            \n",
    "    def read_data(self,link):\n",
    "        #read in the stuff\n",
    "        all_tweets=[]\n",
    "        tot_time=0\n",
    "        for line in open(link):\n",
    "            curr_dat=json.loads(line)\n",
    "            all_tweets.append(curr_dat)\n",
    "        return all_tweets\n",
    "    \n",
    "    def get_clean_datasets(self):\n",
    "        vocab=Counter([c[0] for d in self.all_tweets for c in d])\n",
    "        #cleaned word/tag combos and cleaned tweets:\n",
    "        vocab, clean_tweets=self.add_unks(vocab,3)\n",
    "        pairs=Counter([(a[0],a[1]) for b in clean_tweets for a in b])\n",
    "        return clean_tweets,vocab,pairs\n",
    "        \n",
    "\n",
    "#we need two things in this function:\n",
    "#1. clean the word vocabulary\n",
    "#2. replace words for UNKs in the word/tag pairs for calculaing e(x|y) later \n",
    "    def add_unks(self,vocab, thresh):\n",
    "        all_words=vocab.copy()\n",
    "        low_freq_words=[k for k,v in all_words.items() if v <= thresh]\n",
    "        ###\n",
    "        print('Fraction of total vocabulary replaced with UNKs', len(low_freq_words)/len(all_words))\n",
    "        ddd=sum([all_words[l] for l in low_freq_words])/sum(all_words.values())\n",
    "        print('Fraction of total corpus replaced with UNKs',ddd)\n",
    "        unks=[]\n",
    "        for word in low_freq_words:\n",
    "            unks+=[self.check_unk_type(word)]*all_words[word]\n",
    "            del all_words[word]\n",
    "        all_words+=Counter(unks)\n",
    "        #clean and count the word/tag pairs\n",
    "        clean_tweets=self.cleanse_tweets(all_words)\n",
    "        return all_words, clean_tweets\n",
    "    \n",
    "    def cleanse_tweets(self,vocab):\n",
    "        clean_tweets=copy.deepcopy(self.all_tweets)\n",
    "        for line in clean_tweets:\n",
    "            for pair in line:\n",
    "                if vocab[pair[0]]==0:\n",
    "                    pair[0]=self.check_unk_type(pair[0])\n",
    "        return clean_tweets\n",
    "        \n",
    "\n",
    "    def check_unk_type(self,word):\n",
    "        if(word.isdigit()):\n",
    "            if(len(word)==4 and (word.startswith('1') or word.startswith('2'))):\n",
    "                return 'UNK_YEAR'\n",
    "            else:\n",
    "                return 'UNK_NUMBER'\n",
    "        elif((len(word)>=3 and word[-3]=='.' and word[-2:].isdigit()) or word[0]=='$' or word[-1]=='$'):\n",
    "            return 'UNK_MONEY'\n",
    "        elif(len(word)>=8 and word[:2].isdigit() and word[2]=='/' and word[3:5].isdigit() and word[5]=='/' and word[6:].isdigit()):\n",
    "            return 'UNK_DATE'\n",
    "        elif(word[0]=='#'):\n",
    "            return 'UNK_HASH'\n",
    "        elif(word[0]=='@'):\n",
    "            return 'UNK_AT'\n",
    "        elif(word[:4]=='http' or word[:4]=='www.'):\n",
    "            return 'UNK_LINK'\n",
    "        elif(len(word)==1 and f'U+{ord(word):X}'[:4]=='U+1F'):\n",
    "            return 'UNK_EMOJI'\n",
    "        elif(word.isupper()):\n",
    "            return 'UNK_CAPS'\n",
    "        elif(word[0].isupper()):\n",
    "            return 'UNK_UPCASE'\n",
    "        elif(word.islower()):\n",
    "            return 'UNK_LOWCASE'\n",
    "        else:\n",
    "            return 'UNK_ELSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataset('../HW2/hw2-data/twt.bonus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corp=data.only_words\n",
    "model=Word2Vec(clean_corp,workers=8,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('twitter_opt_try_one.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize?\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=model[model.wv.vocab]\n",
    "pca=PCA(n_components=2)\n",
    "result=pca.fit_transform(X)\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(result[:100,0],result[:100,1])\n",
    "for i, word in enumerate(words[:100]):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
